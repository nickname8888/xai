# Explainable AI

This repository contains code implementations for visualizing and interpreting the predictions of convolutional neural networks using three widely adopted Explainable AI (XAI) techniques:

- Grad-CAM (Gradient-weighted Class Activation Mapping)
  
  ![grad-cam](https://github.com/user-attachments/assets/8419c88b-df34-47b4-baec-9d39124a8d7d)


- LIME (Local Interpretable Model-Agnostic Explanations)

![lime](https://github.com/user-attachments/assets/e11b06a0-0d15-41b9-bb19-ab6868dae2ed)


- SHAP (SHapley Additive exPlanations)

![shap-prediction](https://github.com/user-attachments/assets/92d722d6-b49f-4b8e-8baf-b5150384710d)


Each method helps shed light on which parts of an input image contributed most to the modelâ€™s classification decision.
Run the notebooks in Google Colab for the best results. All dependencies are automatically handled in the notebooks.

For a detailed explanation of each technique and how they compare, read the full blog post here: https://medium.com/@neeraj.nerkar/a-practical-introduction-to-explainable-ai-xai-c659ce3f03f9


